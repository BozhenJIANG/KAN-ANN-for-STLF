{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af2d3224-c396-4c6c-9334-427c96ca463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kan import *\n",
    "import pytz\n",
    "import time\n",
    "import datetime\n",
    "import torch.optim as optim\n",
    "from data_process import data_process_without_norm\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import copy\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "df = data_process_without_norm()\n",
    "df = df.drop(\"entsoe\",axis=1)\n",
    "loc_tz = pytz.timezone('Europe/Zurich')\n",
    "split_date_train_ = loc_tz.localize(datetime.datetime(2016,1,1,0,0,0,0))\n",
    "split_date_train = loc_tz.localize(datetime.datetime(2016,3,1,0,0,0,0))\n",
    "split_date_test = loc_tz.localize(datetime.datetime(2016,3,15,0,0,0,0))\n",
    "\n",
    "df_train_ = df.loc[(split_date_train_ < df.index)]\n",
    "df_train = df_train_.loc[df_train_.index <= split_date_train].copy()\n",
    "_temp_df = df.loc[split_date_test > df.index]\n",
    "df_test = _temp_df.loc[_temp_df.index > split_date_train].copy()\n",
    "\n",
    "# Input standardization\n",
    "scaler_input = StandardScaler()\n",
    "# scaler_input = MinMaxScalerr()\n",
    "_temp_scaled_input_data = scaler_input.fit_transform(df_train.iloc[:,1:])\n",
    "x_train_input = _temp_scaled_input_data\n",
    "\n",
    "#Output standardization\n",
    "scaler_output = StandardScaler()\n",
    "# scaler_input = MinMaxScaler()\n",
    "_temp_scaled_output_data = scaler_output.fit_transform(np.array(df_train.iloc[:,0]).reshape(-1,1))\n",
    "x_train_label = _temp_scaled_output_data\n",
    "\n",
    "#Test set standardization\n",
    "x_test_input = scaler_input.transform(df_test.iloc[:,1:])\n",
    "x_test_label = scaler_output.transform(np.array(df_test.iloc[:,0]).reshape(-1,1))\n",
    "\n",
    "_temp_test_input = np.hstack((x_test_input[:,:8],x_test_input[:,-3:-1]))\n",
    "_temp_test_label = x_test_label\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "_temp_train_input = np.hstack((x_train_input[:, :8], x_train_input[:, -3:-1]))\n",
    "_temp_train_label = x_train_label\n",
    "\n",
    "# 将数据转换为 PyTorch 张量\n",
    "train_input_tensor = torch.tensor(_temp_train_input.astype(np.float32))\n",
    "train_label_tensor = torch.tensor(_temp_train_label.astype(np.float32))\n",
    "\n",
    "test_input_tensor = torch.tensor(_temp_test_input.astype(np.float32))\n",
    "test_label_tensor = torch.tensor(_temp_test_label.astype(np.float32))\n",
    "# 定义 k 折交叉验证\n",
    "k = 5  # 例如，5折交叉验证\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb0b2bd5-b4ca-436e-ba7f-346579722572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimpleCNN 模型\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, conv_out_channels=16, time_step=24):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels=num_inputs, out_channels=conv_out_channels, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(conv_out_channels * time_step, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # 调整输入形状为 (batch_size, num_inputs, time_step)\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        x = x.view(x.size(0), -1)  # 展平\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# SimpleLSTM 模型\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, lstm_hidden_units=16):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=num_inputs, hidden_size=lstm_hidden_units, batch_first=True)\n",
    "        self.fc = nn.Linear(lstm_hidden_units, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]  # 取最后一个时间步的输出\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# CNNLSTM 模型\n",
    "class CNNLSTM(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, conv_out_channels=16, lstm_hidden_units=16, time_step=24):\n",
    "        super(CNNLSTM, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels=num_inputs, out_channels=conv_out_channels, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lstm = nn.LSTM(input_size=conv_out_channels, hidden_size=lstm_hidden_units, batch_first=True)\n",
    "        self.fc = nn.Linear(lstm_hidden_units, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # 调整输入形状为 (batch_size, num_inputs, time_step)\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        x = x.permute(0, 2, 1)  # 调整回 LSTM 所需的输入形状 (batch_size, time_step, conv_out_channels)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]  # 取最后一个时间步的输出\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# SimpleTransformer 模型\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, d_model=16, num_heads=2, num_layers=2):\n",
    "        super(SimpleTransformer, self).__init__()\n",
    "        self.embedding = nn.Linear(num_inputs, d_model)\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(d_model, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# SimpleFCN1 模型\n",
    "class SimpleFCN1(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_units=16):\n",
    "        super(SimpleFCN1, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_units, num_outputs)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# SimpleFCN2 模型\n",
    "class SimpleFCN2(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_units1=32, hidden_units2=16):\n",
    "        super(SimpleFCN2, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_units1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_units1, hidden_units2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_units2, num_outputs)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "# Informer 模型\n",
    "class Informer(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, d_model=16, n_heads=2, e_layers=2, d_layers=1, d_ff=32, \n",
    "                 dropout=0.1, activation='relu', output_attention=False, distil=True):\n",
    "        super(Informer, self).__init__()\n",
    "        self.enc_embedding = DataEmbedding(num_inputs, d_model, dropout)\n",
    "        self.encoder = Encoder(\n",
    "            [\n",
    "                EncoderLayer(\n",
    "                    AttentionLayer(\n",
    "                        ProbAttention(False, factor=5, attention_dropout=dropout, output_attention=False),\n",
    "                        d_model, n_heads),\n",
    "                    d_model,\n",
    "                    d_ff,\n",
    "                    dropout=dropout,\n",
    "                    activation=activation\n",
    "                ) for l in range(e_layers)\n",
    "            ],\n",
    "            norm_layer=torch.nn.LayerNorm(d_model)\n",
    "        )\n",
    "        self.projection = nn.Linear(d_model, num_outputs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.enc_embedding(x)\n",
    "        enc_out, attns = self.encoder(x)\n",
    "        enc_out = enc_out[:, -1, :]  # 取最后一个时间步\n",
    "        output = self.projection(enc_out)\n",
    "        return output\n",
    "\n",
    "# Autoformer 模型\n",
    "class Autoformer(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, d_model=16, n_heads=2, e_layers=2, d_ff=32, \n",
    "                 moving_avg=25, dropout=0.1, activation='relu'):\n",
    "        super(Autoformer, self).__init__()\n",
    "        self.enc_embedding = DataEmbedding(num_inputs, d_model, dropout)\n",
    "        self.encoder = Encoder(\n",
    "            [\n",
    "                EncoderLayer(\n",
    "                    AutoCorrelationLayer(\n",
    "                        AutoCorrelation(False, factor=3, attention_dropout=dropout, output_attention=False),\n",
    "                        d_model, n_heads),\n",
    "                    d_model,\n",
    "                    d_ff,\n",
    "                    moving_avg=moving_avg,\n",
    "                    dropout=dropout,\n",
    "                    activation=activation\n",
    "                ) for l in range(e_layers)\n",
    "            ],\n",
    "            norm_layer=torch.nn.LayerNorm(d_model)\n",
    "        )\n",
    "        self.projection = nn.Linear(d_model, num_outputs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.enc_embedding(x)\n",
    "        enc_out, attns = self.encoder(x)\n",
    "        enc_out = enc_out[:, -1, :]  # 取最后一个时间步\n",
    "        output = self.projection(enc_out)\n",
    "        return output\n",
    "\n",
    "# Informer 和 Autoformer 所需的组件\n",
    "class DataEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model, dropout=0.1):\n",
    "        super(DataEmbedding, self).__init__()\n",
    "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.value_embedding(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        padding = 1\n",
    "        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model, \n",
    "                                  kernel_size=3, padding=padding, padding_mode='circular')\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.tokenConv(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.attn_layers = nn.ModuleList(attn_layers)\n",
    "        self.conv_layers = nn.ModuleList(conv_layers) if conv_layers is not None else None\n",
    "        self.norm = norm_layer\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        attns = []\n",
    "        if self.conv_layers is not None:\n",
    "            for attn_layer, conv_layer in zip(self.attn_layers, self.conv_layers):\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask)\n",
    "                x = conv_layer(x)\n",
    "                attns.append(attn)\n",
    "            x, attn = self.attn_layers[-1](x, attn_mask=attn_mask)\n",
    "            attns.append(attn)\n",
    "        else:\n",
    "            for attn_layer in self.attn_layers:\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask)\n",
    "                attns.append(attn)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        return x, attns\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, attention, d_model, d_ff=None, moving_avg=25, dropout=0.1, activation=\"relu\"):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        self.attention = attention\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
    "        \n",
    "        # Autoformer 特有的组件\n",
    "        if moving_avg > 0:\n",
    "            self.decomp1 = series_decomp(moving_avg)\n",
    "            self.decomp2 = series_decomp(moving_avg)\n",
    "        else:\n",
    "            self.decomp1 = None\n",
    "            self.decomp2 = None\n",
    "            \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        # 注意力机制\n",
    "        new_x, attn = self.attention(x, x, x, attn_mask=attn_mask)\n",
    "        \n",
    "        # Autoformer 的分解机制\n",
    "        if self.decomp1 is not None:\n",
    "            x = x + self.dropout(new_x)\n",
    "            x, _ = self.decomp1(x)\n",
    "        else:\n",
    "            x = x + self.dropout(new_x)\n",
    "\n",
    "        # 前馈网络\n",
    "        y = x\n",
    "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
    "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
    "        \n",
    "        # Autoformer 的分解机制\n",
    "        if self.decomp2 is not None:\n",
    "            res, _ = self.decomp2(x + y)\n",
    "        else:\n",
    "            res = x + y\n",
    "            \n",
    "        return res, attn\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, attention, d_model, n_heads, d_keys=None, d_values=None):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        d_keys = d_keys or (d_model // n_heads)\n",
    "        d_values = d_values or (d_model // n_heads)\n",
    "\n",
    "        self.inner_attention = attention\n",
    "        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.key_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.value_projection = nn.Linear(d_model, d_values * n_heads)\n",
    "        self.out_projection = nn.Linear(d_values * n_heads, d_model)\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask=None):\n",
    "        B, L, _ = queries.shape\n",
    "        _, S, _ = keys.shape\n",
    "        H = self.n_heads\n",
    "\n",
    "        queries = self.query_projection(queries).view(B, L, H, -1)\n",
    "        keys = self.key_projection(keys).view(B, S, H, -1)\n",
    "        values = self.value_projection(values).view(B, S, H, -1)\n",
    "\n",
    "        out, attn = self.inner_attention(\n",
    "            queries, keys, values, attn_mask\n",
    "        )\n",
    "        out = out.view(B, L, -1)\n",
    "\n",
    "        return self.out_projection(out), attn\n",
    "\n",
    "class ProbAttention(nn.Module):\n",
    "    def __init__(self, mask_flag=True, factor=5, scale=None, attention_dropout=0.1, output_attention=False):\n",
    "        super(ProbAttention, self).__init__()\n",
    "        self.factor = factor\n",
    "        self.scale = scale\n",
    "        self.mask_flag = mask_flag\n",
    "        self.output_attention = output_attention\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "\n",
    "    def _prob_QK(self, Q, K, sample_k, n_top):\n",
    "        B, H, L_K, E = K.shape\n",
    "        _, _, L_Q, _ = Q.shape\n",
    "\n",
    "        K_expand = K.unsqueeze(-3).expand(B, H, L_Q, L_K, E)\n",
    "        index_sample = torch.randint(L_K, (L_Q, sample_k))\n",
    "        K_sample = K_expand[:, :, torch.arange(L_Q).unsqueeze(1), index_sample, :]\n",
    "        Q_K_sample = torch.matmul(Q.unsqueeze(-2), K_sample.transpose(-2, -1)).squeeze(-2)\n",
    "\n",
    "        M = Q_K_sample.max(-1)[0] - torch.div(Q_K_sample.sum(-1), L_K)\n",
    "        M_top = M.topk(n_top, sorted=False)[1]\n",
    "\n",
    "        Q_reduce = Q[torch.arange(B)[:, None, None],\n",
    "                   torch.arange(H)[None, :, None],\n",
    "                   M_top, :]\n",
    "        Q_K = torch.matmul(Q_reduce, K.transpose(-2, -1))\n",
    "\n",
    "        return Q_K, M_top\n",
    "\n",
    "    def _get_initial_context(self, V, L_Q):\n",
    "        B, H, L_V, D = V.shape\n",
    "        if not self.mask_flag:\n",
    "            V_sum = V.mean(dim=-2)\n",
    "            contex = V_sum.unsqueeze(-2).expand(B, H, L_Q, V_sum.shape[-1]).clone()\n",
    "        else:\n",
    "            contex = V.cumsum(dim=-2)\n",
    "        return contex\n",
    "\n",
    "    def _update_context(self, context_in, V, scores, index, L_Q, attn_mask):\n",
    "        B, H, L_V, D = V.shape\n",
    "\n",
    "        if self.mask_flag:\n",
    "            attn_mask = ProbMask(B, H, L_Q, index, scores, device=V.device)\n",
    "            scores.masked_fill_(attn_mask.mask, -np.inf)\n",
    "\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "\n",
    "        context_in[torch.arange(B)[:, None, None],\n",
    "        torch.arange(H)[None, :, None],\n",
    "        index, :] = torch.matmul(attn, V).type_as(context_in)\n",
    "        if self.output_attention:\n",
    "            attns = (torch.ones([B, H, L_V, L_V]) / L_V).type_as(attn).to(attn.device)\n",
    "            attns[torch.arange(B)[:, None, None], torch.arange(H)[None, :, None], index, :] = attn\n",
    "            return (context_in, attns)\n",
    "        else:\n",
    "            return (context_in, None)\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask=None):\n",
    "        B, L_Q, H, D = queries.shape\n",
    "        _, L_K, _, _ = keys.shape\n",
    "\n",
    "        queries = queries.transpose(2, 1)\n",
    "        keys = keys.transpose(2, 1)\n",
    "        values = values.transpose(2, 1)\n",
    "\n",
    "        U_part = self.factor * np.ceil(np.log(L_K)).astype('int').item()\n",
    "        u = self.factor * np.ceil(np.log(L_Q)).astype('int').item()\n",
    "\n",
    "        U_part = U_part if U_part < L_K else L_K\n",
    "        u = u if u < L_Q else L_Q\n",
    "\n",
    "        scores_top, index = self._prob_QK(queries, keys, sample_k=U_part, n_top=u)\n",
    "\n",
    "        scale = self.scale or 1. / math.sqrt(D)\n",
    "        if scale is not None:\n",
    "            scores_top = scores_top * scale\n",
    "\n",
    "        context = self._get_initial_context(values, L_Q)\n",
    "        context, attn = self._update_context(context, values, scores_top, index, L_Q, attn_mask)\n",
    "\n",
    "        return context.transpose(2, 1).contiguous(), attn\n",
    "\n",
    "class AutoCorrelation(nn.Module):\n",
    "    def __init__(self, mask_flag=True, factor=1, scale=None, attention_dropout=0.1, output_attention=False):\n",
    "        super(AutoCorrelation, self).__init__()\n",
    "        self.factor = factor\n",
    "        self.scale = scale\n",
    "        self.mask_flag = mask_flag\n",
    "        self.output_attention = output_attention\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "\n",
    "    def time_delay_agg_training(self, values, corr):\n",
    "        batch, head, channel, length = values.shape\n",
    "        # 寻找top-k的相关性\n",
    "        top_k = int(self.factor * math.log(length))\n",
    "        mean_value = torch.mean(torch.mean(corr, dim=1), dim=1)\n",
    "        index = torch.topk(torch.mean(mean_value, dim=0), top_k, dim=-1)[1]\n",
    "        weights = torch.stack([mean_value[:, index[i]] for i in range(top_k)], dim=-1)\n",
    "        # 更新聚合的值\n",
    "        tmp_values = values\n",
    "        delays_agg = torch.zeros_like(values).float()\n",
    "        for i in range(top_k):\n",
    "            pattern = torch.roll(tmp_values, -int(index[i]), -1)\n",
    "            delays_agg = delays_agg + pattern * \\\n",
    "                         (weights[:, i].unsqueeze(1).unsqueeze(1).unsqueeze(1).repeat(1, head, channel, length))\n",
    "        return delays_agg\n",
    "\n",
    "    def time_delay_agg_inference(self, values, corr):\n",
    "        batch, head, channel, length = values.shape\n",
    "        # 寻找top-k的相关性\n",
    "        top_k = int(self.factor * math.log(length))\n",
    "        mean_value = torch.mean(torch.mean(corr, dim=1), dim=1)\n",
    "        weights, delay = torch.topk(mean_value, top_k, dim=-1)\n",
    "        # 更新聚合的值\n",
    "        tmp_values = values\n",
    "        delays_agg = torch.zeros_like(values).float()\n",
    "        for i in range(top_k):\n",
    "            pattern = torch.roll(tmp_values, -int(delay[0, i]), -1)\n",
    "            delays_agg = delays_agg + pattern * \\\n",
    "                         (weights[:, i].unsqueeze(1).unsqueeze(1).unsqueeze(1).repeat(1, head, channel, length))\n",
    "        return delays_agg\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask=None):\n",
    "        B, L, H, E = queries.shape\n",
    "        _, S, _, D = values.shape\n",
    "\n",
    "        # 周期性自相关\n",
    "        q_fft = torch.fft.rfft(queries.permute(0, 2, 3, 1).contiguous(), dim=-1)\n",
    "        k_fft = torch.fft.rfft(keys.permute(0, 2, 3, 1).contiguous(), dim=-1)\n",
    "        res = q_fft * torch.conj(k_fft)\n",
    "        corr = torch.fft.irfft(res, dim=-1)\n",
    "\n",
    "        if self.training:\n",
    "            V = self.time_delay_agg_training(values.permute(0, 2, 3, 1).contiguous(), corr).permute(0, 3, 1, 2)\n",
    "        else:\n",
    "            V = self.time_delay_agg_inference(values.permute(0, 2, 3, 1).contiguous(), corr).permute(0, 3, 1, 2)\n",
    "\n",
    "        if self.output_attention:\n",
    "            return (V.contiguous(), corr.permute(0, 3, 1, 2))\n",
    "        else:\n",
    "            return (V.contiguous(), None)\n",
    "\n",
    "class AutoCorrelationLayer(nn.Module):\n",
    "    def __init__(self, correlation, d_model, n_heads, d_keys=None, d_values=None):\n",
    "        super(AutoCorrelationLayer, self).__init__()\n",
    "        d_keys = d_keys or (d_model // n_heads)\n",
    "        d_values = d_values or (d_model // n_heads)\n",
    "\n",
    "        self.inner_attention = correlation\n",
    "        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.key_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.value_projection = nn.Linear(d_model, d_values * n_heads)\n",
    "        self.out_projection = nn.Linear(d_values * n_heads, d_model)\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask=None):\n",
    "        B, L, _ = queries.shape\n",
    "        _, S, _ = keys.shape\n",
    "        H = self.n_heads\n",
    "\n",
    "        queries = self.query_projection(queries).view(B, L, H, -1)\n",
    "        keys = self.key_projection(keys).view(B, S, H, -1)\n",
    "        values = self.value_projection(values).view(B, S, H, -1)\n",
    "\n",
    "        out, attn = self.inner_attention(\n",
    "            queries, keys, values, attn_mask\n",
    "        )\n",
    "        out = out.view(B, L, -1)\n",
    "\n",
    "        return self.out_projection(out), attn\n",
    "\n",
    "class series_decomp(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super(series_decomp, self).__init__()\n",
    "        self.moving_avg = MovingAvg(kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        moving_mean = self.moving_avg(x)\n",
    "        res = x - moving_mean\n",
    "        return res, moving_mean\n",
    "\n",
    "class MovingAvg(nn.Module):\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super(MovingAvg, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "class ProbMask():\n",
    "    def __init__(self, B, H, L, index, scores, device=\"cpu\"):\n",
    "        _mask = torch.ones(L, scores.shape[-1], dtype=torch.bool).to(device).triu(1)\n",
    "        _mask_ex = _mask[None, None, :].expand(B, H, L, scores.shape[-1])\n",
    "        indicator = _mask_ex[torch.arange(B)[:, None, None],\n",
    "                    torch.arange(H)[None, :, None],\n",
    "                    index, :].to(device)\n",
    "        self._mask = indicator.view(scores.shape).to(device)\n",
    "\n",
    "    @property\n",
    "    def mask(self):\n",
    "        return self._mask\n",
    "\n",
    "# 定义损失函数\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "# 超参数范围\n",
    "learning_rates = [0.01, 0.005, 0.001]\n",
    "batch_sizes = [128, 64, 32]\n",
    "\n",
    "# K折交叉验证\n",
    "k = 3\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "def create_time_series_data(data, time_step):\n",
    "    num_samples = data.shape[0]\n",
    "    num_features = data.shape[1]\n",
    "    time_series_data = []\n",
    "\n",
    "    for i in range(num_samples - time_step + 1):\n",
    "        time_series_data.append(data[i:i + time_step].clone().detach().numpy())\n",
    "\n",
    "    return torch.tensor(np.array(time_series_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b5e10ff-ae26-4503-b9a6-9babc92e6806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 超参数范围\n",
    "# learning_rates = [0.01, 0.005, 0.001]\n",
    "# batch_sizes = [128, 64, 32]\n",
    "# num_epochs_list = [50, 100]\n",
    "# hidden_units_list = [16, 32]\n",
    "\n",
    "# def create_time_series_data(data, time_step):\n",
    "#     num_samples = data.shape[0]\n",
    "#     num_features = data.shape[1]\n",
    "#     time_series_data = []\n",
    "\n",
    "#     for i in range(num_samples - time_step + 1):\n",
    "#         time_series_data.append(data[i:i + time_step].clone().detach().numpy())\n",
    "\n",
    "#     return torch.tensor(np.array(time_series_data))\n",
    "\n",
    "# def train_and_evaluate_model(model_class, num_inputs, num_outputs, train_input_tensor, train_label_tensor, time_step=24):\n",
    "#     results = []\n",
    "#     for lr in learning_rates:\n",
    "#         for batch_size in batch_sizes:\n",
    "#             for num_epochs in num_epochs_list:\n",
    "#                 for hidden_units in hidden_units_list:\n",
    "#                     fold_results = []\n",
    "#                     for fold, (train_index, val_index) in enumerate(kf.split(train_input_tensor)):\n",
    "#                         # 创建模型实例\n",
    "#                         if model_class in [SimpleCNN, CNNLSTM]:\n",
    "#                             model = model_class(num_inputs, num_outputs, hidden_units, time_step)\n",
    "#                         else:\n",
    "#                             model = model_class(num_inputs, num_outputs, hidden_units)\n",
    "\n",
    "#                         optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "#                         # 分割数据\n",
    "#                         train_input_fold = train_input_tensor[train_index]\n",
    "#                         train_label_fold = train_label_tensor[train_index]\n",
    "#                         val_input_fold = train_input_tensor[val_index]\n",
    "#                         val_label_fold = train_label_tensor[val_index]\n",
    "\n",
    "#                         # 如果模型是CNN或LSTM，转换数据为时间序列格式\n",
    "#                         if issubclass(model_class, (SimpleCNN, SimpleLSTM, CNNLSTM)):\n",
    "#                             train_input_fold = create_time_series_data(train_input_fold, time_step)\n",
    "#                             train_label_fold = train_label_fold[time_step - 1:]  # 对齐标签\n",
    "#                             val_input_fold = create_time_series_data(val_input_fold, time_step)\n",
    "#                             val_label_fold = val_label_fold[time_step - 1:]  # 对齐标签\n",
    "\n",
    "#                         # 创建数据加载器\n",
    "#                         train_loader = torch.utils.data.DataLoader(\n",
    "#                             dataset=torch.utils.data.TensorDataset(train_input_fold.clone().detach(), train_label_fold.clone().detach()),\n",
    "#                             batch_size=batch_size,\n",
    "#                             shuffle=True\n",
    "#                         )\n",
    "#                         val_loader = torch.utils.data.DataLoader(\n",
    "#                             dataset=torch.utils.data.TensorDataset(val_input_fold.clone().detach(), val_label_fold.clone().detach()),\n",
    "#                             batch_size=batch_size,\n",
    "#                             shuffle=False\n",
    "#                         )\n",
    "\n",
    "#                         # 训练模型\n",
    "#                         start_time = time.time()\n",
    "#                         for epoch in range(num_epochs):\n",
    "#                             model.train()\n",
    "#                             for step, (x, y) in enumerate(train_loader):\n",
    "#                                 output = model(x)\n",
    "#                                 loss = loss_func(output, y)\n",
    "#                                 optimizer.zero_grad()\n",
    "#                                 loss.backward()\n",
    "#                                 optimizer.step()\n",
    "\n",
    "#                         # 验证模型\n",
    "#                         model.eval()\n",
    "#                         val_loss = 0\n",
    "#                         with torch.no_grad():\n",
    "#                             for x, y in val_loader:\n",
    "#                                 output = model(x)\n",
    "#                                 loss = loss_func(output, y)\n",
    "#                                 val_loss += loss.item()\n",
    "#                         val_loss /= len(val_loader)\n",
    "#                         fold_results.append(val_loss)\n",
    "#                         end_time = time.time()\n",
    "#                         print(f\"Fold {fold + 1}, LR: {lr}, Batch Size: {batch_size}, Epochs: {num_epochs}, Hidden Units: {hidden_units}, Validation Loss: {val_loss:.4f}, Time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "#                     # 计算当前超参数组合在所有折上的平均损失\n",
    "#                     avg_val_loss = np.mean(fold_results)\n",
    "#                     results.append({\n",
    "#                         'learning_rate': lr,\n",
    "#                         'batch_size': batch_size,\n",
    "#                         'num_epochs': num_epochs,\n",
    "#                         'hidden_units': hidden_units,\n",
    "#                         'avg_val_loss': avg_val_loss\n",
    "#                     })\n",
    "\n",
    "#     # 转换结果为DataFrame以便分析\n",
    "#     results_df = pd.DataFrame(results)\n",
    "\n",
    "#     # 找到最佳配置\n",
    "#     best_config = results_df.loc[results_df['avg_val_loss'].idxmin()]\n",
    "\n",
    "#     print(f\"\\nBest Configuration for {model_class.__name__}: {best_config}\")\n",
    "#     return results_df\n",
    "      \n",
    "# # 评估每个模型\n",
    "# cnn_results = train_and_evaluate_model(SimpleCNN, num_inputs=10, num_outputs=1, train_input_tensor=train_input_tensor, train_label_tensor=train_label_tensor)\n",
    "# lstm_results = train_and_evaluate_model(SimpleLSTM, num_inputs=10, num_outputs=1, train_input_tensor=train_input_tensor, train_label_tensor=train_label_tensor)\n",
    "# cnn_lstm_results = train_and_evaluate_model(CNNLSTM, num_inputs=10, num_outputs=1, train_input_tensor=train_input_tensor, train_label_tensor=train_label_tensor)\n",
    "# transformer_results = train_and_evaluate_model(SimpleTransformer, num_inputs=10, num_outputs=1, train_input_tensor=train_input_tensor, train_label_tensor=train_label_tensor)\n",
    "# fcn1_results = train_and_evaluate_model(SimpleFCN1, num_inputs=10, num_outputs=1, train_input_tensor=train_input_tensor, train_label_tensor=train_label_tensor)\n",
    "# fcn2_results = train_and_evaluate_model(SimpleFCN2, num_inputs=10, num_outputs=1, train_input_tensor=train_input_tensor, train_label_tensor=train_label_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edd5dddc-7688-4d4c-a712-c4324fb2cc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df = pd.DataFrame(cnn_results)\n",
    "# results_df.to_csv('./parameters_configuration/cnn_BaseConfig-SPRING.csv') \n",
    "# results_df = pd.DataFrame(lstm_results)\n",
    "# results_df.to_csv('./parameters_configuration/lstm_BaseConfig-SPRING.csv') \n",
    "# results_df = pd.DataFrame(cnn_lstm_results)\n",
    "# results_df.to_csv('./parameters_configuration/cnn_lstm_BaseConfig-SPRING.csv') \n",
    "# results_df = pd.DataFrame(transformer_results)\n",
    "# results_df.to_csv('./parameters_configuration/transformer_BaseConfig-SPRING.csv') \n",
    "# results_df = pd.DataFrame(fcn1_results)\n",
    "# results_df.to_csv('./parameters_configuration/fcn1_BaseConfig-SPRING.csv') \n",
    "# results_df = pd.DataFrame(fcn2_results)\n",
    "# results_df.to_csv('./parameters_configuration/fcn2_BaseConfig-SPRING.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c272d4d3-e3f1-4e43-8a5b-e77a9841fcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "results_df_cnn = pd.read_csv('./parameters_configuration/cnn_BaseConfig-SPRING.csv',index_col=0)\n",
    "results_df_lstm = pd.read_csv('./parameters_configuration/lstm_BaseConfig-SPRING.csv',index_col=0)\n",
    "results_df_cnn_lstm = pd.read_csv('./parameters_configuration/cnn_lstm_BaseConfig-SPRING.csv',index_col=0)\n",
    "results_df_transformer = pd.read_csv('./parameters_configuration/transformer_BaseConfig-SPRING.csv',index_col=0)\n",
    "results_df_fcn1 = pd.read_csv('./parameters_configuration/fcn1_BaseConfig-SPRING.csv',index_col=0)\n",
    "results_df_fcn2 = pd.read_csv('./parameters_configuration/fcn2_BaseConfig-SPRING.csv',index_col=0)\n",
    "results_df_Informer = pd.read_csv('./parameters_configuration/Informer_BaseConfig-SPRING.csv',index_col=0)\n",
    "results_df_Autoformer = pd.read_csv('./parameters_configuration/Autoformer_BaseConfig-SPRING.csv',index_col=0)\n",
    "\n",
    "def train_and_test_model(model_class, learning_rate, batch_size, num_epochs, hidden_units, train_input_tensor, train_label_tensor, test_input_tensor, test_label_tensor, scaler_output, num_inputs=10, num_outputs=1, time_step=24):\n",
    "    results = []\n",
    "\n",
    "    # 如果模型是CNN或LSTM，转换数据为时间序列格式\n",
    "    if issubclass(model_class, (SimpleCNN, SimpleLSTM, CNNLSTM, Informer, Autoformer)):\n",
    "        train_input_tensor = create_time_series_data(train_input_tensor, time_step)\n",
    "        train_label_tensor = train_label_tensor[time_step - 1:]  # 对齐标签\n",
    "        test_input_tensor = create_time_series_data(test_input_tensor, time_step)\n",
    "        test_label_tensor = test_label_tensor[time_step - 1:]  # 对齐标签\n",
    "        learning_rate = learning_rate*0.1\n",
    "    # 创建数据加载器\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=torch.utils.data.TensorDataset(train_input_tensor.clone().detach(), train_label_tensor.clone().detach()),\n",
    "        batch_size=int(batch_size),\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # 初始化模型、损失函数和优化器\n",
    "    model = model_class(num_inputs, num_outputs, hidden_units)\n",
    "    loss_func = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # 训练模型\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for step, (x, y) in enumerate(train_loader):\n",
    "            output = model(x)\n",
    "            loss = loss_func(output, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # 验证模型\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(test_input_tensor.clone().detach())\n",
    "        output_inverse = scaler_output.inverse_transform(output.numpy())\n",
    "        test_label_inverse = scaler_output.inverse_transform(test_label_tensor.numpy())\n",
    "\n",
    "        # 计算评估指标\n",
    "        rmse = np.sqrt(mean_squared_error(test_label_inverse, output_inverse))\n",
    "        mae = mean_absolute_error(test_label_inverse, output_inverse)\n",
    "        mape = np.mean(np.abs((test_label_inverse - output_inverse) / test_label_inverse)) * 100\n",
    "\n",
    "        test_loss = loss_func(output, test_label_tensor.clone().detach()).item()\n",
    "\n",
    "    # print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    results.append({\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'MAPE': mape\n",
    "    })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbf43259-c6eb-44e4-b9f4-322a65a53d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Test Results: [{'RMSE': 402.27502, 'MAE': 329.6201, 'MAPE': 4.347362741827965}]\n",
      "123.7829978466034\n",
      "LSTM Test Results: [{'RMSE': 433.89166, 'MAE': 339.44846, 'MAPE': 4.458225145936012}]\n",
      "38.21889066696167\n",
      "CNN-LSTM Test Results: [{'RMSE': 415.73938, 'MAE': 344.46292, 'MAPE': 4.536542668938637}]\n",
      "188.3872127532959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\23124862r\\AppData\\Local\\anaconda3\\envs\\kan\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer Test Results: [{'RMSE': 680.3065, 'MAE': 505.82855, 'MAPE': 6.341973692178726}]\n",
      "554.6263198852539\n",
      "FCN1 Test Results: [{'RMSE': 386.18146, 'MAE': 311.89343, 'MAPE': 4.135579243302345}]\n",
      "6.648909091949463\n",
      "FCN2 Test Results: [{'RMSE': 347.53235, 'MAE': 277.79965, 'MAPE': 3.656603768467903}]\n",
      "8.081424951553345\n",
      "Informer Test Results: [{'RMSE': 433.17636, 'MAE': 336.19913, 'MAPE': 4.484019428491592}]\n",
      "706.3486950397491\n",
      "Autoformer Test Results: [{'RMSE': 623.0429, 'MAE': 534.8849, 'MAPE': 7.020276784896851}]\n",
      "523.3416652679443\n"
     ]
    }
   ],
   "source": [
    "# 找到每个模型的最佳配置\n",
    "def get_best_config(results_df):\n",
    "    best_config = results_df.loc[results_df['avg_val_loss'].idxmin(), ['learning_rate', 'batch_size', 'num_epochs', 'hidden_units']]\n",
    "    return {\n",
    "        'learning_rate': best_config['learning_rate'],\n",
    "        'batch_size': int(best_config['batch_size']),\n",
    "        'num_epochs': int(best_config['num_epochs']),\n",
    "        'hidden_units': int(best_config['hidden_units'])\n",
    "    }\n",
    "\n",
    "# 获取每个模型的最佳配置\n",
    "best_config_cnn = get_best_config(results_df_cnn)\n",
    "best_config_lstm = get_best_config(results_df_lstm)\n",
    "best_config_cnn_lstm = get_best_config(results_df_cnn_lstm)\n",
    "best_config_transformer = get_best_config(results_df_transformer)\n",
    "best_config_fcn1 = get_best_config(results_df_fcn1)\n",
    "best_config_fcn2 = get_best_config(results_df_fcn2)\n",
    "best_config_Informer = get_best_config(results_df_Informer)\n",
    "best_config_Autoformer = get_best_config(results_df_Autoformer)\n",
    "\n",
    "# 假设 train_input_tensor, train_label_tensor, test_input_tensor, test_label_tensor 已经定义\n",
    "\n",
    "# 训练和测试每个模型，并打印结果\n",
    "import time\n",
    "start_time = time.time()\n",
    "cnn_test_results = train_and_test_model(SimpleCNN, **best_config_cnn, train_input_tensor=train_input_tensor, train_label_tensor=train_label_tensor, test_input_tensor=test_input_tensor, test_label_tensor=test_label_tensor, scaler_output=scaler_output)\n",
    "print(\"CNN Test Results:\", cnn_test_results)\n",
    "end_time = time.time()\n",
    "print(end_time-start_time)\n",
    "\n",
    "start_time = time.time()\n",
    "lstm_test_results = train_and_test_model(SimpleLSTM, **best_config_lstm, train_input_tensor=train_input_tensor, train_label_tensor=train_label_tensor, test_input_tensor=test_input_tensor, test_label_tensor=test_label_tensor, scaler_output=scaler_output)\n",
    "print(\"LSTM Test Results:\", lstm_test_results)\n",
    "end_time = time.time()\n",
    "print(end_time-start_time)\n",
    "\n",
    "start_time = time.time()\n",
    "cnn_lstm_test_results = train_and_test_model(CNNLSTM, **best_config_cnn_lstm, train_input_tensor=train_input_tensor, train_label_tensor=train_label_tensor, test_input_tensor=test_input_tensor, test_label_tensor=test_label_tensor, scaler_output=scaler_output)\n",
    "print(\"CNN-LSTM Test Results:\", cnn_lstm_test_results)\n",
    "end_time = time.time()\n",
    "print(end_time-start_time)\n",
    "\n",
    "start_time = time.time()\n",
    "transformer_test_results = train_and_test_model(SimpleTransformer, **best_config_transformer, train_input_tensor=train_input_tensor, train_label_tensor=train_label_tensor, test_input_tensor=test_input_tensor, test_label_tensor=test_label_tensor, scaler_output=scaler_output)\n",
    "print(\"Transformer Test Results:\", transformer_test_results)\n",
    "end_time = time.time()\n",
    "print(end_time-start_time)\n",
    "\n",
    "start_time = time.time()\n",
    "fcn1_test_results = train_and_test_model(SimpleFCN1, **best_config_fcn1, train_input_tensor=train_input_tensor, train_label_tensor=train_label_tensor, test_input_tensor=test_input_tensor, test_label_tensor=test_label_tensor, scaler_output=scaler_output)\n",
    "print(\"FCN1 Test Results:\", fcn1_test_results)\n",
    "end_time = time.time()\n",
    "print(end_time-start_time)\n",
    "\n",
    "start_time = time.time()\n",
    "fcn2_test_results = train_and_test_model(SimpleFCN2, **best_config_fcn2, train_input_tensor=train_input_tensor, train_label_tensor=train_label_tensor, test_input_tensor=test_input_tensor, test_label_tensor=test_label_tensor, scaler_output=scaler_output)\n",
    "print(\"FCN2 Test Results:\", fcn2_test_results)\n",
    "end_time = time.time()\n",
    "print(end_time-start_time)\n",
    "\n",
    "start_time = time.time()\n",
    "Informer_test_results = train_and_test_model(Informer, **best_config_Informer, train_input_tensor=train_input_tensor, train_label_tensor=train_label_tensor, test_input_tensor=test_input_tensor, test_label_tensor=test_label_tensor, scaler_output=scaler_output)\n",
    "print(\"Informer Test Results:\", Informer_test_results)\n",
    "end_time = time.time()\n",
    "print(end_time-start_time)\n",
    "\n",
    "start_time = time.time()\n",
    "Autoformer_test_results = train_and_test_model(Autoformer, **best_config_Autoformer, train_input_tensor=train_input_tensor, train_label_tensor=train_label_tensor, test_input_tensor=test_input_tensor, test_label_tensor=test_label_tensor, scaler_output=scaler_output)\n",
    "print(\"Autoformer Test Results:\", Autoformer_test_results)\n",
    "end_time = time.time()\n",
    "print(end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae2f93b3-85ff-4c2f-bfc8-babba9bb951e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = dict()\n",
    "result[\"cnn\"] = cnn_test_results\n",
    "result[\"lstm\"] = lstm_test_results\n",
    "result[\"cnn_lstm\"] = cnn_lstm_test_results\n",
    "result[\"transformer\"] = transformer_test_results\n",
    "result[\"fcn1\"] = fcn1_test_results\n",
    "result[\"fcn2\"] = fcn2_test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bc1a112-0412-43ec-9e92-8ed54cdea277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cnn': [{'RMSE': 402.27502, 'MAE': 329.6201, 'MAPE': 4.347362741827965}],\n",
       " 'lstm': [{'RMSE': 433.89166, 'MAE': 339.44846, 'MAPE': 4.458225145936012}],\n",
       " 'cnn_lstm': [{'RMSE': 415.73938,\n",
       "   'MAE': 344.46292,\n",
       "   'MAPE': 4.536542668938637}],\n",
       " 'transformer': [{'RMSE': 680.3065,\n",
       "   'MAE': 505.82855,\n",
       "   'MAPE': 6.341973692178726}],\n",
       " 'fcn1': [{'RMSE': 386.18146, 'MAE': 311.89343, 'MAPE': 4.135579243302345}],\n",
       " 'fcn2': [{'RMSE': 347.53235, 'MAE': 277.79965, 'MAPE': 3.656603768467903}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ab565f-006e-42ab-9d63-94f0a4cc62e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kan-kernel",
   "language": "python",
   "name": "kan-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
